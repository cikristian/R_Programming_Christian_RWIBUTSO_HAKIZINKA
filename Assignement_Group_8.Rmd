---
title: "Assignment_Group_8"
author: "Group_8"
date: "`r Sys.Date()`"
output: html_document
---
# Q1
## Sub question 1.
1. Which probability distribution of a random variable ğ‘‹, say, better describing the 
marks of the students in descriptive statistics? (Give the statistical standing reasons 
for your answer) 
```{r}
# Create marks vector in R
marks <- c(
  6.89, 10.35, 14.5, 15.35, 13.4, 12.35, 13.6, 11.3, 4.6, 15.85, 10.15, 8.15, 8.25,
  17.05, 15.2, 16.2, 15.05, 16.25, 18.5, 16.8, 15.25, 12.75, 12.45, 10.2, 13.3, 8.7,
  13.05, 10.9, 13, 13.85, 11.55, 14.5, 12.85, 17.15, 13.3, 15.25, 17.55, 11.25, 12.85,
  14.5, 10.7, 11.9, 14, 14.6, 12.4, 15.35, 12.3, 11.7, 11.7, 10, 11.8, 15.6,
  8.6, 10, 12.5, 9.9, 10.3, 13.3, 14, 16.5, 14, 14.7, 18.2, 15.1, 14.4,
  8.1, 12.4, 10.9, 13.1, 14.5, 15.4, 10, 13.2, 13.4, 14.6, 17.5, 13.4, 15.8,
  13.2, 13.3, 13.8, 11.2, 15.3, 16.1, 9.3, 16.5, 11.5, 13, 14.7, 9.3, 8.6,
  12.4, 12.8, 10.8, 5.5, 13.5, 12.4, 11.8, 12.2, 8.5, 9.4, 11.9, 8.6, 13.2,
  12.4, 11.5, 10.1, 14.8, 12.7, 6.1, 8.6, 9.3, 9.9, 8.4, 11.3, 13.1, 13.5,
  4.1, 7.5, 13.9, 15.1, 13.2, 14.5, 17.6, 14.7, 12.5, 15.4, 15.5, 9.2, 10.5,
  13.45, 18.15, 10.9, 12.65, 13.2, 17.6, 13.9, 12.4, 14.6, 12.4, 17.15, 16.4, 13.75,
  14.4, 17.35, 14.3, 14.65, 16.1, 11.25, 10.35, 13.95, 12.55, 7.6, 14.95, 9.6, 18.35,
  12.2, 12.35, 16.7, 15.3, 9, 14.05, 14.65, 11.15, 17.45, 4.75, 13.6, 14.4, 15.2,
  8.75, 13.4, 13.6, 14.35, 12.65, 14.9, 15.1, 15.45, 16.55, 9.75, 16.9, 14.35, 12.66,
  11.44, 11.47, 12.1, 11.34, 10.88, 8.34, 13.24, 15.59, 14.09, 12.4, 15.84, 12.39, 17.64,
  12.78, 12.78, 15.41, 11.21, 12.56, 10.85, 16.04, 11.1, 13.25, 15.9, 14.5, 12.42, 11,
  10.08, 12.8, 8.18, 15.75, 13.26, 12.61, 13.98, 6.77, 13.85, 12.42, 15.36, 13.24, 13.88,
  17.7, 15.02, 17.22, 16.98, 16.12, 17.66, 17.64, 13.32, 17.14, 11.6, 18.1, 14.13, 14.98,
  16.44, 18.73, 14.82, 16.02, 14.66, 14.48, 12.56, 10.21, 18.72, 19.42, 16.22, 17.07, 15.82,
  15.27
)

length(marks)
summary(marks)
```

### Nature of the data (descriptive assessment)
Let X denote the studentsâ€™ marks.
From inspection of the data set X is continuous (decimal values) and it is bounded ( between 4.10 and 19.42)
These summary immediately rule out all Discrete distributions (Binomial, Poisson, Geometric). 

### Checking the distribution
```{r}
# Histogram with density 
hist(marks, probability = T, col = 'lightblue',main = "Histogram of Studentsi' marks")
# Add the density estimate
lines(density(marks), col = 'red', lwd = 2)
```
### Ploting the density 


```{r}
plot( density(marks), main = "Density Plot of Students' Marks",
  xlab = "Marks", lwd = 2, col = "red")

```
The histogram and density plot indicates that the distribution of studentsâ€™ marks is approximately Normal. The data exhibit a single central peak, approximate symmetry, and smooth tails, which are characteristic features of a Gaussian/normal distribution. Minor deviations from perfect normality are expected due to the bounded nature of marks and assessment constraints. Therefore, modeling the marks using a Normal distribution can be statistically justified.

### Check the Normal Q-Q plot

```{r}
qqnorm(marks, main = "Normal Q-Q Plot of Marks")
qqline(marks, col ='red', lwd = 2)
```
The Normal Qâ€“Q plot shows an almost linear pattern, with only minor deviations at the extremes. This indicates that the distribution of studentsâ€™ marks is approximately Normal, particularly in the central region. The slight tail deviations are consistent with bounded exam data and do not invalidate the Normal approximation.

```{r}
mu <- mean(marks)
med <- median(marks)

plot(density(marks), main = "Density Plot with Mean and Median", xlab = "Marks", lwd = 2, col = "blue")
abline(v = mu, col = "red", lwd = 2, lty = 2)
abline(v = med, col = "darkgreen", lwd = 2, lty = 3)
legend("topright",
       legend = c(paste("Mean =", round(mu, 2)),paste("Median =", round(med, 2))),
       col = c("red", "darkgreen"),lwd = 2, lty = c(2, 3),bty = "n")

```
For a Gaussian (Normal) distribution Xâˆ¼N(Î¼,Ïƒ2), the mean and median coincide due to the symmetry of the distribution. 
In empirical data, these two measures are expected to be approximately equal. 
For the given data set, the sample mean is 13.19 and the median is 13.30, which are very close. 
This near equality indicates symmetry in the data and provides further evidence that the data set is well approximated by a Normal distribution.

### Further test 
```{r}
#Shapiroâ€“Wilk test
shapiro.test(marks)
```
The Shapiroâ€“Wilk test rejects the null hypothesis of exact normality (p = 0.0056). However, graphical diagnostics (density and Qâ€“Q plots) and descriptive statistics indicate that the distribution is approximately Normal. The rejection is likely due to the sensitivity of the test to minor deviations and the bounded nature of exam scores. Therefore, the Normal distribution remains a reasonable approximation for descriptive and modeling purposes.

## Conclusion

After examining the distribution of the data by plotting the density of the studentsâ€™ marks, the distribution appears bell-shaped. The sample mean and median are approximately equal, indicating symmetry in the data. In addition, the Normal Qâ€“Q plot shows that most of the observations lie close to the reference line, with only minor deviations at the tails. Based on these graphical and descriptive analyses, we conclude that the data set is approximately Normally distributed.

## Subquestion 2.
Generating a random sample of the same size as the size of ğ‘‹ using the established
probability distribution ğ‘“(ğ‘¥). The size of the data set is 248
```{r}
# The normal random generation in R is rnorm()
n <- length(marks)
mu <- mean(marks)
sigma <- sqrt(var(marks))

set.seed(123)     #For reproducibility 
# The new data set that foll the mark distribution
new_marks <- rnorm(n,mean = mu,sd = sigma)
# Computing the summary 
summary(new_marks)
mean(new_marks)
var(new_marks)
```
The generated random sample has a mean and median close to those of the original data set, indicating that it preserves the central tendency and symmetry of the data. Minor deviations at the extremes, such as values exceeding the maximum possible mark, are expected due to the unbounded nature of the Normal distribution and do not invalidate the simulation. Therefore, the random sample is a valid realization from the estimated probability distribution f(x).

```{r}
# Parameters
mu <- mean(new_marks)
median <- median(new_marks)

# Histogram with density
hist(
  new_marks,
  probability = TRUE,
  col = "lightgray",
  main = "Histogram and Density of Generated Marks",
  xlab = "Marks"
)
# Density curve
lines(density(new_marks), col = "blue", lwd = 2)
# Mean (mu)
abline(v = mu, col = "red", lwd = 2, lty = 2)
# Median
abline(v = median, col = "darkgreen", lwd = 2, lty = 3)

legend("topright",
       legend = c(paste("Mean =", round(mu, 2)),paste("Median =", round(median, 2))),
       col = c("red", "darkgreen"),lwd = 2, lty = c(2, 3),bty = "n")

```
## Sub question 4: Fitting Test 
 
4. Carry out a test of goodness of fit to test whether the random sample generated in 
the above sub-question 3 fits the values of the average marks of the students in 
descriptive statistics. Interpret the result of this test. 

```{r}
#ks.test(new_marks, marks)
ks.test(new_marks, marks)
```
A two-sample Kolmogorovâ€“Smirnov goodness-of-fit test was conducted to compare the generated random sample with the observed studentsâ€™ marks. The test yielded a p-value of 0.1058, which is greater than the 5% significance level. Therefore, the null hypothesis was not rejected, indicating no significant difference between the two distributions. This suggests that the generated sample provides a good fit to the observed marks and adequately represents their distribution in descriptive statistics.


# Q2
Consider the dataset â€œrock,â€ which can be directly explored in R using the data() function. 
Generate from the features of this data set a normal multivariate sample in the large data 
set of  ğ‘› = 500 records. 

```{r}
# Load dataset
data(rock)

# Convert to numeric matrix
X <- as.matrix(rock)

# Mean vector (length = 4)
mu <- colMeans(X)

# Covariance matrix (4 x 4) â€” THIS IS THE FIX
Sigma <- cov(X)

# Check dimensions
#length(mu)     
#dim(Sigma)     

# Generate multivariate normal sample
library(MASS)
set.seed(123)

n <- 500
new_rock <- mvrnorm(n = n, mu = mu, Sigma = Sigma)

# Convert to data frame
new_rock <- as.data.frame(new_rock)
# Checking the new sample dimensions 
dim(new_rock)

```

# Q3
Donâ€™t forget to revise the last studied methods for random generation: (Accept-reject 
method, Transformation methods, and sum and mixture. Take any question of your choice 
from your textbooks. (Rizzo or Robert) 
## Transformation Method
3.8 Write a function to generate random variates from a Lognormal(Âµ,Ïƒ)
distribution using a transformation method, and generate a random
sample of size 1000. Compare the histogram with the lognormal density
curve given by the dlnorm function in R.
```{r}
rlognormal_trans <- function(n, mu, sigma) {
  Z <- rnorm(n)                # Step 1: standard normal
  X <- exp(mu + sigma * Z)     # Step 2: transformation
  return(X)
}

```

```{r}
set.seed(123)

n <- 1000
mu <- 0
sigma <- 1

x <- rlognormal_trans(n, mu, sigma)

```

```{r}
hist(x, probability = TRUE, 
     breaks = 30,
     main = "Lognormal Distribution: Simulation vs Density",
     xlab = "x")

curve(dlnorm(x, meanlog = mu, sdlog = sigma),
      col = "red", lwd = 2, add = TRUE)

```
#Q4

### 6.15 Obtain the stratified importance sampling estimate in Example 6.14 and compare it with the result of Example 6.11.

1.Crude Monte Carlo (Example 6.11)

```{r}
set.seed(1)

n <- 100000
x <- rnorm(n)
I_MC <- mean(x^2 * (x >= 1))

I_MC

```
#### 2.Importance Sampling (Example 6.14)

```{r}
set.seed(1)

n <- 100000
u <- runif(n)
x <- qnorm(u * (1 - pnorm(1)) + pnorm(1))
I_IS <- (1 - pnorm(1)) * mean(x^2)

I_IS

```
#### 3.Stratified Importance Sampling (Example 6.15)

```{r}
# Divide samples equally among 3 strata.

set.seed(1)
n <- 100000
n1 <- n2 <- n3 <- n/3

# step 1: Compute stratum probabilities.

p1 <- pnorm(2) - pnorm(1)
p2 <- pnorm(3) - pnorm(2)
p3 <- 1 - pnorm(3)

# Step 2: Function to sample truncated normal

rtrunc <- function(n, a, b) {
  u <- runif(n)
  qnorm(u * (pnorm(b) - pnorm(a)) + pnorm(a))
}
# Step 3: Sample each stratum

# Forces samples to come from each part of the tail.

x1 <- rtrunc(n1, 1, 2)
x2 <- rtrunc(n2, 2, 3)
x3 <- rtrunc(n3, 3, Inf)
# Step 4: Combine estimates

I_SIS <- p1 * mean(x1^2) +
         p2 * mean(x2^2) +
         p3 * mean(x3^2)

I_SIS


```
### In conclusion:

1.The crude Monte Carlo estimator has high variance because most normal samples fall below 1 and contribute zero.
2.Importance sampling improves efficiency by sampling from a truncated normal distribution on 
[1,âˆ).
3.Stratified importance sampling further reduces variance by dividing the tail region into strata and sampling each separately, ensuring better coverage of the integration domain.


